{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11300928,"sourceType":"datasetVersion","datasetId":7067224}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  CMS E2E Classification â€“ Evaluation Test (GSoC 2025)\nThis notebook presents an End-to-End deep learning pipeline to classify CMS detector images of photons and electrons using a custom ResNet-15 model.\n\n- **Dataset**: 32x32 CMS image crops with 2 channels (energy, time)\n- **Model**: ResNet-15-like CNN\n- **Goal**: Achieve high classification accuracy distinguishing photons vs electrons","metadata":{}},{"cell_type":"markdown","source":"##  Load the CMS HDF5 Dataset\n\nWe load the raw image crops for photons and electrons from the provided HDF5 files.  \nEach file contains:\n- `X`: CMS image tensors with shape `(N, 2, 32, 32)`  \n- `y`: Corresponding labels (0 = Photon, 1 = Electron)\n","metadata":{}},{"cell_type":"code","source":"import h5py\nimport numpy as np\n\n# Load HDF5 files for photons and electrons from Kaggle input path\nphotons_file = h5py.File(\"/kaggle/input/data-data/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5\", \"r\")\nelectrons_file = h5py.File(\"/kaggle/input/data-data/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5\", \"r\")\n\n# Print available keys to understand the structure of the files\nprint(\"Photon keys:\", list(photons_file.keys()))\nprint(\"Electron keys:\", list(electrons_file.keys()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:36:50.850213Z","iopub.execute_input":"2025-04-06T18:36:50.850436Z","iopub.status.idle":"2025-04-06T18:36:51.073922Z","shell.execute_reply.started":"2025-04-06T18:36:50.850416Z","shell.execute_reply":"2025-04-06T18:36:51.073063Z"}},"outputs":[{"name":"stdout","text":"Photon keys: ['X', 'y']\nElectron keys: ['X', 'y']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"##  Prepare the CMS Dataset for Training\n\nWe load the image tensors (`X`) and labels (`y`) for both photons and electrons.  \nThen we:\n- Explicitly assign labels: `0` for photons, `1` for electrons\n- Combine both datasets into a single array for training\n\nThis gives us a balanced dataset with equal representation from both classes.\n","metadata":{}},{"cell_type":"code","source":"# Load the image arrays and labels from HDF5 files\nX_photons = np.array(photons_file[\"X\"])  # Shape: (N_photons, 2, 32, 32)\ny_photons = np.array(photons_file[\"y\"])  # Initially may contain default values\n\nX_electrons = np.array(electrons_file[\"X\"])\ny_electrons = np.array(electrons_file[\"y\"])\n\n# Explicitly assign labels for binary classification\ny_photons[:] = 0  # Class 0: Photon\ny_electrons[:] = 1  # Class 1: Electron\n\n# Combine photon and electron samples\nX = np.concatenate([X_photons, X_electrons], axis=0)\ny = np.concatenate([y_photons, y_electrons], axis=0)\n\n# Confirm the final dataset shape and label distribution\nprint(f\"Total samples: {X.shape[0]}, Labels: {np.unique(y, return_counts=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:39:26.749491Z","iopub.execute_input":"2025-04-06T18:39:26.749822Z","iopub.status.idle":"2025-04-06T18:39:43.950963Z","shell.execute_reply.started":"2025-04-06T18:39:26.749793Z","shell.execute_reply":"2025-04-06T18:39:43.950194Z"}},"outputs":[{"name":"stdout","text":"Total samples: 498000, Labels: (array([0., 1.], dtype=float32), array([249000, 249000]))\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"##  Data Preparation and Splitting\n\nNow that we have all the CMS data loaded, we:\n- Convert the NumPy arrays into PyTorch tensors\n- Split the data into training (80%) and testing (20%) sets using stratified sampling\n- Wrap the data into `TensorDataset` objects and use `DataLoader` to handle batching during training and evaluation\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Convert combined data into PyTorch tensors\nX_tensor = torch.tensor(X, dtype=torch.float32)  # Shape: (N, 2, 32, 32)\ny_tensor = torch.tensor(y, dtype=torch.long)     # Labels: 0 or 1\n\n# Perform stratified train-test split (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_tensor, y_tensor, test_size=0.2, stratify=y_tensor, random_state=42\n)\n\n# Create dataset wrappers\ntrain_dataset = TensorDataset(X_train, y_train)\ntest_dataset = TensorDataset(X_test, y_test)\n\n# Define DataLoaders for mini-batch training and evaluation\ntrain_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n\nprint(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:43:38.259241Z","iopub.execute_input":"2025-04-06T18:43:38.259529Z","iopub.status.idle":"2025-04-06T18:43:46.186146Z","shell.execute_reply.started":"2025-04-06T18:43:38.259511Z","shell.execute_reply":"2025-04-06T18:43:46.185198Z"}},"outputs":[{"name":"stdout","text":"Train size: 398400, Test size: 99600\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"##  Tensor Cleanup & Efficient DataLoaders\n\nTo avoid warnings and improve efficiency:\n- We properly detach and clone tensors to prevent shared computation graphs.\n- We re-wrap them using `TensorDataset` and define `DataLoader`s.\n- We also enable multi-threaded loading (`num_workers=2`) and use `pin_memory=True` for faster GPU transfers.\n","metadata":{}},{"cell_type":"code","source":"## ðŸ§¹ Tensor Cleanup & Efficient DataLoaders\n\nTo avoid warnings and improve efficiency:\n- We properly detach and clone tensors to prevent shared computation graphs.\n- We re-wrap them using `TensorDataset` and define `DataLoader`s.\n- We also enable multi-threaded loading (`num_workers=2`) and use `pin_memory=True` for faster GPU transfers.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:55:03.135024Z","iopub.execute_input":"2025-04-06T18:55:03.135372Z","iopub.status.idle":"2025-04-06T18:55:05.122392Z","shell.execute_reply.started":"2025-04-06T18:55:03.135347Z","shell.execute_reply":"2025-04-06T18:55:05.121721Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"##  Dataset Preparation & DataLoader Creation\n\nWe wrap the training and test tensors into `TensorDataset` objects and create `DataLoader` instances to handle batching and shuffling.\n- `batch_size`: Defines the number of samples processed at once. We set it to 128 for optimal performance, but it can be increased if using a GPU.\n- `shuffle=True`: Ensures the training data is shuffled to avoid learning patterns based on order.\n- `shuffle\n","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\n\n# Set batch size for training and testing\nbatch_size = 128  # Increase for GPU acceleration if needed\n\n# Create datasets from tensors\ntrain_dataset = TensorDataset(X_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n\n# Create DataLoaders to handle batching and shuffling\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T18:59:52.154441Z","iopub.execute_input":"2025-04-06T18:59:52.154772Z","iopub.status.idle":"2025-04-06T18:59:52.159196Z","shell.execute_reply.started":"2025-04-06T18:59:52.154746Z","shell.execute_reply":"2025-04-06T18:59:52.158405Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"##  Model Architecture: ResNet15\n\nIn this section, we define the `ResNet15` model with a custom `BasicBlock` used in the ResNet architecture. The model consists of:\n- **BasicBlock**: A basic building block with two convolutional layers, batch normalization, and shortcut connections (residual connections).\n- **ResNet15**: A modified version of ResNet with 15 layers, including multiple `BasicBlocks` for feature extraction and a fully connected layer at the end for classification.\n\n#### Key components:\n- **Convolutional layers**: Learn spatial features from the input data.\n- **Batch Normalization**: Stabilizes and accelerates the training process.\n- **Residual connections**: Help in avoiding vanishing gradients by allowing the network to learn identity mappings.\n","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# BasicBlock: Building block for ResNet\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, stride=1):\n        super(BasicBlock, self).__init__()\n        # Convolutional layers\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        # Shortcut connection to match the input and output dimensions\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_channels != out_channels:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_channels)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))  # Apply first convolution + ReLU + BatchNorm\n        out = self.bn2(self.conv2(out))  # Apply second convolution + BatchNorm\n        out += self.shortcut(x)  # Add shortcut connection\n        out = F.relu(out)  # Final ReLU activation\n        return out\n\n# ResNet15: Main model with residual layers\nclass ResNet15(nn.Module):\n    def __init__(self, num_classes=2):\n        super(ResNet15, self).__init__()\n        self.in_channels = 32\n\n        # Initial convolution layer with batch norm\n        self.conv1 = nn.Conv2d(2, 32, kernel_size=3, stride=1, padding=1,_\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:03:16.775341Z","iopub.execute_input":"2025-04-06T19:03:16.775706Z","iopub.status.idle":"2025-04-06T19:03:16.786922Z","shell.execute_reply.started":"2025-04-06T19:03:16.775679Z","shell.execute_reply":"2025-04-06T19:03:16.786159Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:03:52.973616Z","iopub.execute_input":"2025-04-06T19:03:52.973904Z","iopub.status.idle":"2025-04-06T19:03:53.061013Z","shell.execute_reply.started":"2025-04-06T19:03:52.973882Z","shell.execute_reply":"2025-04-06T19:03:53.060319Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"model = ResNet15().to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:03:57.021227Z","iopub.execute_input":"2025-04-06T19:03:57.021510Z","iopub.status.idle":"2025-04-06T19:03:57.278200Z","shell.execute_reply.started":"2025-04-06T19:03:57.021490Z","shell.execute_reply":"2025-04-06T19:03:57.277444Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"##  Model Training Loop\n\nIn this section, we define the training loop for our `ResNet15` model. The model undergoes training for multiple epochs, where:\n- **Inputs** are fetched in batches.\n- **Loss** is calculated based on the predictions of the model.\n- **Backpropagation** is performed to update the model's weights using the optimizer.\n- The **running loss** for each epoch is computed to monitor the progress of training.\n\nThe input shape is adjusted with `inputs.permute(0, 3, 1, 2)` to match the expected format (channels, height, width).\n","metadata":{}},{"cell_type":"code","source":"# Number of epochs for training\nnum_epochs = 30\n\n# Training loop\nfor epoch in range(num_epochs):\n    model.train()  # Set the model to training mode\n    running_loss = 0.0  # Initialize running loss for the epoch\n\n    for inputs, labels in train_loader:  # Loop through training batches\n        inputs, labels = inputs.to(device), labels.to(device)  # Send data to the device (GPU/CPU)\n\n        # Fix the input shape to match the expected format (batch_size, channels, height, width)\n        inputs = inputs.permute(0, 3, 1, 2)\n\n        optimizer.zero_grad()  # Zero the gradients to prevent accumulation from previous steps\n        outputs = model(inputs)  # Perform forward pass (model prediction)\n        loss = criterion(outputs, labels)  # Compute loss between model output and true labels\n        loss.backward()  # Backpropagate the gradients\n        optimizer.step()  # Update model parameters using the optimizer\n\n        running_loss += loss.item()  # Accumulate loss for averaging later\n\n    avg_loss = running_loss / len(train_loader)  # Calculate average loss for the epoch\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")  # Print the loss for this epoch\n\nprint(\"Training Complete\")  # Indicate that the training is finished","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T19:05:42.723275Z","iopub.execute_input":"2025-04-06T19:05:42.723705Z","iopub.status.idle":"2025-04-06T20:38:00.232305Z","shell.execute_reply.started":"2025-04-06T19:05:42.723665Z","shell.execute_reply":"2025-04-06T20:38:00.231560Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/30, Loss: 0.6151\nEpoch 2/30, Loss: 0.5700\nEpoch 3/30, Loss: 0.5585\nEpoch 4/30, Loss: 0.5530\nEpoch 5/30, Loss: 0.5486\nEpoch 6/30, Loss: 0.5450\nEpoch 7/30, Loss: 0.5418\nEpoch 8/30, Loss: 0.5394\nEpoch 9/30, Loss: 0.5377\nEpoch 10/30, Loss: 0.5350\nEpoch 11/30, Loss: 0.5331\nEpoch 12/30, Loss: 0.5315\nEpoch 13/30, Loss: 0.5292\nEpoch 14/30, Loss: 0.5271\nEpoch 15/30, Loss: 0.5251\nEpoch 16/30, Loss: 0.5226\nEpoch 17/30, Loss: 0.5201\nEpoch 18/30, Loss: 0.5174\nEpoch 19/30, Loss: 0.5145\nEpoch 20/30, Loss: 0.5109\nEpoch 21/30, Loss: 0.5065\nEpoch 22/30, Loss: 0.5019\nEpoch 23/30, Loss: 0.4962\nEpoch 24/30, Loss: 0.4895\nEpoch 25/30, Loss: 0.4814\nEpoch 26/30, Loss: 0.4722\nEpoch 27/30, Loss: 0.4618\nEpoch 28/30, Loss: 0.4507\nEpoch 29/30, Loss: 0.4378\nEpoch 30/30, Loss: 0.4248\nTraining Complete\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"##  Model Evaluation\n\nAfter training the model, we evaluate its performance on the test set. We set the model to evaluation mode with `model.eval()`, ensuring that batch normalization and dropout layers work in inference mode. \n\nThe following steps are performed:\n- **Prediction**: For each batch in the test set, the model predicts the class (photon or electron).\n- **Accuracy**: The accuracy score is calculated, and a **classification report** is printed to assess precision, recall, f1-score, and support for each class.\n","metadata":{}},{"cell_type":"code","source":"# Set the model to evaluation mode (inference mode)\nmodel.eval()\nall_preds = []  # List to store predictions\nall_labels = []  # List to store true labels\n\n# Disable gradient calculation to speed up inference and save memory\nwith torch.no_grad():\n    for inputs, labels in test_loader:  # Loop through test data\n        inputs, labels = inputs.to(device), labels.to(device)  # Send data to device (GPU/CPU)\n        inputs = inputs.permute(0, 3, 1, 2)  # Fix input shape [B, C, H, W]\n        outputs = model(inputs)  # Perform forward pass (model prediction)\n        preds = torch.argmax(outputs, dim=1)  # Get class predictions (photon or electron)\n        \n        # Store the predictions and true labels on the CPU (for evaluation)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\n# Calculate accuracy score\nfrom sklearn.metrics import accuracy_score, classification_report\n\nacc = accuracy_score(all_labels, all_preds)  # Compute accuracy\nprint(f\"Test Accuracy: {acc:.4f}\")\n\n# Print detailed classification metrics\nprint(\"\\nClassification Report:\\n\", classification_report(all_labels, all_preds, target_names=[\"Photon\", \"Electron\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T20:47:51.827575Z","iopub.execute_input":"2025-04-06T20:47:51.828012Z","iopub.status.idle":"2025-04-06T20:48:02.071181Z","shell.execute_reply.started":"2025-04-06T20:47:51.827981Z","shell.execute_reply":"2025-04-06T20:48:02.070414Z"}},"outputs":[{"name":"stdout","text":"Test Accuracy: 0.7171\n\nClassification Report:\n               precision    recall  f1-score   support\n\n      Photon       0.71      0.74      0.72     49800\n    Electron       0.73      0.69      0.71     49800\n\n    accuracy                           0.72     99600\n   macro avg       0.72      0.72      0.72     99600\nweighted avg       0.72      0.72      0.72     99600\n\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"##  Saving the Model\n\nAfter training and evaluating the model, it is saved to disk using `torch.save()`. This allows you to load the model later for inference or further training. The model's **state dictionary** (which contains the weights and biases of the model) is saved to a `.pth` file.\n\nIn this case, we save the trained `ResNet15` model to a file named `resnet15_cms.pth`.","metadata":{}},{"cell_type":"code","source":"torch.save(model.state_dict(), \"resnet15_cms.pth\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-06T20:50:03.447188Z","iopub.execute_input":"2025-04-06T20:50:03.447481Z","iopub.status.idle":"2025-04-06T20:50:03.471833Z","shell.execute_reply.started":"2025-04-06T20:50:03.447462Z","shell.execute_reply":"2025-04-06T20:50:03.471143Z"}},"outputs":[],"execution_count":18}]}